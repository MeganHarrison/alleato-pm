# Quick Start Guide - Procore Support Documentation Crawl

**Ready to crawl?** Follow these simple steps.

---

## âš¡ Quick Start (3 Steps)

### Step 1: Navigate to Directory
```bash
cd scripts/screenshot-capture
```

### Step 2: Install Dependencies (if needed)
```bash
npm install
```

### Step 3: Run the Crawl
```bash
node scripts/crawl-support-comprehensive.js
```

That's it! The crawler will:
- ğŸŒ Open a browser window
- ğŸ“¸ Visit and capture support documentation pages
- ğŸ’¾ Save screenshots, DOM, and metadata
- ğŸ“Š Generate comprehensive reports including sitemap.md

---

## ğŸ“ What Gets Created

After the crawl completes, you'll have:

```
procore-support-crawl/
â”œâ”€â”€ pages/                              # Individual page captures
â”‚   â”œâ”€â”€ [page-name]/
â”‚   â”‚   â”œâ”€â”€ screenshot.png              # Full-page screenshot
â”‚   â”‚   â”œâ”€â”€ dom.html                    # Complete HTML
â”‚   â”‚   â””â”€â”€ metadata.json               # Structured data
â”‚   â””â”€â”€ [50-100+ more pages...]
â”‚
â””â”€â”€ reports/                            # Generated reports
    â”œâ”€â”€ sitemap.md                      # â­ MAIN DELIVERABLE
    â”œâ”€â”€ detailed-report.json            # Complete JSON data
    â”œâ”€â”€ link-graph.json                 # Page relationships
    â””â”€â”€ crawl-summary.json              # Statistics
```

---

## ğŸ¯ Main Deliverable

**The sitemap.md file is your primary deliverable.**

It contains:
- âœ… Complete table of contents with all pages
- âœ… Detailed statistics and category breakdown
- âœ… Individual page listings with full details
- âœ… Content structure and navigation hierarchy
- âœ… Links to all screenshots
- âœ… Quick reference index

**Location:** `reports/sitemap.md`

---

## â±ï¸ How Long Will It Take?

- **50 pages:** 30-45 minutes
- **100 pages:** 60-120 minutes
- **Maximum (100+ pages):** 2-3 hours

The crawler has a safety limit of 100 pages by default. You can modify this in the script if needed.

---

## ğŸ‘€ What You'll See

While running, the console will show:
```
ğŸš€ Starting Procore Support Documentation Crawl...
ğŸ“ Starting URL: https://support.procore.com/products/online

ğŸ“¸ Capturing: products/online
   URL: https://support.procore.com/products/online
   âœ… Captured: 45 links, 12 clickables, 3 expandables

ğŸ¯ Looking for expandable sections on: products/online
   ğŸ” Found expandable: "Getting Started"
   ğŸ“‹ Found 5 revealed items

ğŸ“Š Progress: 1/100 pages captured, 45 in queue

ğŸ“¸ Capturing: products/online/project-management
   ...
```

---

## âœ… How to Know It's Done

The crawler finishes when you see:

```
âœ… Crawl complete!
ğŸ“ Output directory: ./procore-support-crawl
ğŸ“Š Total pages captured: 87
ğŸ”— Total links discovered: 1,243

ğŸ“Š Generating comprehensive reports...
âœ… Sitemap generated: procore-support-crawl/reports/sitemap.md
âœ… Reports generated in: procore-support-crawl/reports
```

---

## ğŸ” Next Steps After Crawl

1. **View the sitemap:**
   ```bash
   open procore-support-crawl/reports/sitemap.md
   ```

2. **Check statistics:**
   ```bash
   cat procore-support-crawl/reports/crawl-summary.json
   ```

3. **Browse screenshots:**
   ```bash
   open procore-support-crawl/pages/
   ```

4. **Start analysis:**
   - Review [IMPLEMENTATION-TASKS.md](IMPLEMENTATION-TASKS.md) for next steps
   - Use sitemap.md to navigate captured documentation
   - Extract features from detailed-report.json

---

## ğŸ› ï¸ Troubleshooting

### Browser doesn't open
**Issue:** Script runs but no browser appears
**Solution:** Check that Playwright is installed:
```bash
npm install
npx playwright install
```

### Out of disk space
**Issue:** Error about disk space during crawl
**Solution:** Free up at least 500MB, or reduce MAX_PAGES in the script

### Pages not loading
**Issue:** Some pages fail to capture
**Solution:** Increase WAIT_TIME in the script (line 7) from 2000 to 3000+

### Crawl seems stuck
**Issue:** No progress for several minutes
**Solution:** Check the browser window - might be waiting for user interaction

---

## âš™ï¸ Configuration

Edit `scripts/crawl-support-comprehensive.js` to adjust:

```javascript
// Line 7: Wait time between pages (milliseconds)
const WAIT_TIME = 2000;  // Increase if pages load slowly

// Line 419: Maximum pages to crawl
const maxPages = 100;  // Increase for more comprehensive crawl
```

---

## ğŸ“ Questions?

- **Where's the main output?** â†’ `reports/sitemap.md`
- **How do I find specific features?** â†’ Search sitemap.md
- **Can I stop and resume?** â†’ Yes, already-visited URLs are tracked
- **How do I re-crawl?** â†’ Delete `pages/*` and `reports/*` then re-run

---

## ğŸ“š Full Documentation

For complete details, see:
- [README.md](README.md) - Comprehensive documentation
- [IMPLEMENTATION-TASKS.md](IMPLEMENTATION-TASKS.md) - Task breakdown
- [CRAWL-SUMMARY.md](CRAWL-SUMMARY.md) - Results summary (after crawl)

---

**Ready?** Run the command:

```bash
cd scripts/screenshot-capture && node scripts/crawl-support-comprehensive.js
```
